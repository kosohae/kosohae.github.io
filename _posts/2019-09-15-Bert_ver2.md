---
layout: post
title: 논문 리뷰 - language model
subtitle: BERT 논문 이해 및 내용 정리2
bigimg: ./img/path.jpg
category: Research
tags: [deeplearning, NLP]
---
# BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding

# version 2. fine tuned 
- 본 내용은 사내 발표를 위해 논문 핵심 정리한 내용으로 공유합니다. 정정이 필요한 부분은 메일 부탁드립니다!


# 4. Experiments
4.1 GLUE

General Language Understanding Evaluation

use the final hidden vector C ( [CLS] ) as the aggregation representation

KxH (K - number of labels) weights

standard classification loss C and W

log(Softmax(CW^T)

fine tuning learning rate :  5e-5, 4e-5, 3e-5, and 2e-5

BERT Large의 경우 small dataset의 경우 unstable => 이럴 땐 random start 사용했다고 되어있음.

data shuffling, classification layer initialization에 따라서 결과가 달라질 수 있음.

# 5. Effective of model size

differ number of layers, hidden units, attention heads

BASE BERT : 110M, LARGE BERT: 235M

large scale task : machine translation, language modeling

다른 조건 동일, Layer 개수 3->6->12 모든 성능이 개선.

옆의 표만 참고하면, Layer와 attention Head와 Hidden size는 크면 클수록 결과는 좋게 나옴.

BERT가 Underfitting일까.

<img src="20190915/BERT_ version20.png" width=465px />

<img src="20190915/BERT_ version21.png" width=500px />

# Future Work

* BERT fine tuning - QNLI (relationship task)  , KorQuAD 1.0
  * test 90.8 acc (QNLI) , multi-lingual EM 18.6, F1 34.7
* Mini MLM implement
  * dataset : squad 2.0
  * tokenizing : sentence piece model
  * model : Bert base model, transformer
  * input : [CLS]A[SEP]B 구조 형성
  * output : vocab x
* Plus : Fine tuning dive
  * spanBERT : Improving Pre-training by Representing and Predicting Spans

# 4. Experiments
4.2 SQuAD

Given a question and a passage from Wiki containing answer

task : predict the answer text span in the passage

